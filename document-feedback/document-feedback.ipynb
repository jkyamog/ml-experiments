{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Challenge\n",
    "Tackling an interesting problem: given a user query, search through a PDF document and provide feedback on how well the query aligns with the document's content.\n",
    "\n",
    "# The Solution\n",
    "The approach is a three-step process: Load & Index, Search & RAG, and Feedback Generation.\n",
    "\n",
    "## Load & Index\n",
    "First, I need to understand the PDF document. I do this by creating a \"semantic index\". It's like creating a map of the document, but instead of landmarks, we have vectors.\n",
    "\n",
    "## Search & RAG\n",
    "Next, I take your query and find the most related parts in the PDF document. This is where RAG (Retrieval Augmentation Generation) comes in. It's like giving the system a cheat sheet before the big test.\n",
    "\n",
    "## Feedback Generation\n",
    "Finally, I generate feedback for you. This isn't just a simple \"yes\" or \"no\". I provide detailed feedback with references from the PDF document. It's like having footnotes for your query.\n",
    "\n",
    "# Caveats and Considerations\n",
    "- **AI and LLMs are still evolving**. Theyâ€™re like teenagers - unpredictable and constantly changing. So, sometimes, they might give you surprising outputs.\n",
    "- **Quality of feedback** depends on your query. Too short or too vague, and you might not get accurate feedback. Too long or complex, and you might confuse the system. For optimal results, aim for the Goldilocks zone - a few sentences to about 1 paragraph. Not too short, not too long.\n",
    "- **Feedback may be slow**. It might take a few seconds before the LLM responds.\n",
    "\n",
    "# Dive Deeper\n",
    "The code is open for you to explore. Feel free to fork it, and see how it fits your use case. I'll break down each section and highlight key points.\n",
    "\n",
    "If you have any suggestions or improvements, don't hesitate to share. For more technical details, continue reading along this Jupyter notebook.  \n",
    "\n",
    "I started to develop this with a Jupiter Notebook, then implemented into fastapi backend and sveltekit frontend.  I then re-organized and rewritten the Jupiter notebook to distill and explain the different essential parts.  I also changed the implementation from AzureOpenAI to OpenAI, AzureSearch to Chroma.  Langchain's wrappers around them make it easy swap the implementations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import depedencies and init environment. Have a .env file with following vars:\n",
    "\n",
    "`\n",
    "OPENAI_API_KEY=XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My example below uses OWASP Web Security Testing Guideline 4.2 PDF, tells the LLM that it is an expert cyber security expert.  You can play around with different data, variables below are the likely ones that is needed to be changed when the data is changed.  The llm_personality and llm_reference_instruction will be incorporated to a system prompt.  These are important prompt engineering as it gives context to the LLM, the LLM can also create the reference links.  In this example it only displays the page numbers, it can be more complex than this like creating html links to the reference document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'wstg-v4.2.pdf'\n",
    "\n",
    "topic = 'OWASP Web Security Testing Guideline'\n",
    "\n",
    "llm_personality = f'You are a web cyber security expert. Upon searching the {topic}, provide details of Test such as: Summary, Objectives, How to Test and Remediation'\n",
    "\n",
    "llm_reference_instructions = \"An example is 'Web Security T esting Guide v4.2445', the page is 445 as it the digits after v4.2.  Put in the reference Page: 445\"\n",
    "\n",
    "user_query = \"\"\"\n",
    "Testing Login\n",
    "\n",
    "1. Go to the login page, login as the test user.\n",
    "2. Try to login with incorrect password\n",
    "3. After successful login with correct password check profile page is correct.\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key components to be initialized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loader = PyPDFLoader(f'./pdfs/{filename}')\n",
    "\n",
    "embeddings: OpenAIEmbeddings = OpenAIEmbeddings(model='text-embedding-ada-002')\n",
    "\n",
    "vector_store = Chroma(filename, embeddings, persist_directory=\".chroma/\")\n",
    "\n",
    "openai = OpenAI(\n",
    "    api_key=os.environ['OPENAI_API_KEY']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load & Index the pdf into the vector database.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "documents = loader.load_and_split()\n",
    "\n",
    "vector_store.add_documents(documents=documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search the index using the vectorized documents and query.  Flatten the search results to be used as a RAG for the LLM.  This may return more results than what is needed, however let the LLM filter through the content on a later stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query):\n",
    "\n",
    "    docs = vector_store.similarity_search(\n",
    "        query=query,\n",
    "        k=5 # returns 5 documents, play around this value what works better.  Too little means some information is ommitted, too much may mean too big of a context\n",
    "    )\n",
    "    return docs\n",
    "\n",
    "delimiter = \"\\n\\n\"\n",
    "\n",
    "\n",
    "\n",
    "page_contents = [result.page_content for result in search(user_query)]\n",
    "search_results_flattened = delimiter.join(page_contents)\n",
    "\n",
    "page_contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrap a system prompt and use the search results as a RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = f\"\"\"\n",
    "{llm_personality}\n",
    "\n",
    "The following search results in order of descending relevance:\n",
    "\n",
    "{search_results_flattened}\n",
    "\n",
    "Based on the information retrieved, please provide a response to the user's query.  \n",
    "\n",
    "Since there are several possible matches, use footnotes style to clearly mark which reference that matched was taken from. \n",
    "Specify the only the page taken from in the reference.  The page number extracted from the search results starting words. \n",
    "{llm_reference_instructions}\n",
    "\"\"\"\n",
    "\n",
    "user_prompt = f\"\"\"Does user query follow {topic}? \n",
    "\n",
    "user query below:\n",
    "\n",
    "{user_query}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "completions = openai.chat.completions.create(model=\"gpt-4-turbo-preview\",\n",
    "                                                  messages=[\n",
    "                                                      {\n",
    "                                                          \"role\": \"system\",\n",
    "                                                          \"content\": system_prompt\n",
    "                                                      },\n",
    "                                                      {\n",
    "                                                        \"role\": \"user\",\n",
    "                                                        \"content\": user_prompt,\n",
    "                                                      }\n",
    "                                                  ])\n",
    "completions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are using jupyter notebooks using the IPython.display would render the content much more better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "display(Markdown(completions.choices[0].message.content))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
